{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.6 (default, Jan  8 2020, 19:59:22) \\n[GCC 7.3.0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T05:53:48.166724Z",
     "start_time": "2020-03-22T05:53:48.126889Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T05:53:55.123666Z",
     "start_time": "2020-03-22T05:53:48.462116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape is : (40000, 1)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/w_review_train.csv',header=None)\n",
    "train_ori = pd.read_csv('../data/w_review_train.csv',header=None)\n",
    "train.columns = ['review']\n",
    "train_ori.columns = ['review']\n",
    "print(f\"train shape is : {train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-19T04:23:14.453962Z",
     "start_time": "2020-01-19T04:23:14.447957Z"
    }
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T05:53:56.593658Z",
     "start_time": "2020-03-22T05:53:56.519751Z"
    }
   },
   "outputs": [],
   "source": [
    "import pythainlp\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T05:56:15.120398Z",
     "start_time": "2020-03-22T05:56:15.008282Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_special_char(train):\n",
    "\n",
    "    special_char = r\"[\\\\\\@\\^\\%\\(\\*\\|\\<\\`\\.\\_\\=\\!\\>\\;\\?\\#\\$\\)\\/\\,\\ๆ\\'\\~\\:\\+\\-\\\"\\฿]\"\n",
    "    space_char = r\"[\\n\\&]\"\n",
    "    num = '[0-9]'\n",
    "    num_th = '[๑๒๓๓๔๕๖๗๘๙]'\n",
    "    eng_words = '[A-Za-z]'\n",
    "    \n",
    "    train['review'] = train['review'].str.replace(special_char, ' ')\n",
    "    train['review'] = train['review'].str.replace(space_char, ' ')\n",
    "    train['review'] = train['review'].str.replace(num, ' ', regex=True)\n",
    "    train['review'] = train['review'].str.replace(num_th, ' ', regex=True)\n",
    "    train['review'] = train['review'].str.replace(eng_words, ' ', regex=True)\n",
    "    train['review'] = train['review'].str.lower()\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 11min 6s, sys: 2min 25s, total: 2h 13min 32s\n",
      "Wall time: 16min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = clean_special_char(train)\n",
    "train['tokenized'] = train['review'].apply(lambda x: word_tokenize(x,engine='attacut',keep_whitespace=False, custom_dict=trie))\n",
    "train.to_pickle('tokenized_with_wongnai_customdict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(file_nm):\n",
    "    bigram_custom = pd.read_csv(file_nm,header=None,usecols=[0])\n",
    "    bigram_custom = bigram_custom[0].tolist()\n",
    "    return bigram_custom\n",
    "\n",
    "# top words dict\n",
    "bigram_dict = load_dict('../dict/bigram_customdict_top100.txt')\n",
    "trigram_dict = load_dict('../dict/trigram_customdict_top1000.txt')\n",
    "quadgram_dict = load_dict('../dict/quadgram_customdict_top300.txt')\n",
    "top_words_customdict = set(bigram_dict + trigram_dict + quadgram_dict)\n",
    "\n",
    "# food dict\n",
    "custom_dict_wongnai = pd.read_csv('../data/food_dictionary.txt',header=None,names=['word'])\n",
    "food_dict = custom_dict_wongnai['word'].tolist()\n",
    "custom_dict_wongnai = set(food_dict)\n",
    "\n",
    "# # union\n",
    "# custom_dict = custom_dict_wongnai.union(top_words_customdict)\n",
    "# trie = pythainlp.tokenize.dict_trie(dict_source=custom_dict)\n",
    "\n",
    "# # if use deepcut directly\n",
    "# tokenize_dict = list(trie.words)\n",
    "# tokenize_dict = sorted(tokenize_dict, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ดีนะ', 'ดีมาก', 'รอคิว', 'มาลอง', 'คอหมู'],\n",
       " ['ไก่ต้นตำรับบุญตงกี่ เนื้อน่อง สะโพก',\n",
       "  'ไก่ย่าง เนื้อปลากระพงทอดผัดเหล้าจีน',\n",
       "  'ข้าวไข่กระทะหน้ากุ้งกระเทียมพริกไทย',\n",
       "  'chicken schnitzel with italian slaw',\n",
       "  'ทอดมันปลาหมึก แหนมเนือง กุ้งพันอ้อย'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample sorted list\n",
    "tokenize_dict[:5], tokenize_dict[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_dict = food_dict + bigram_dict + trigram_dict + quadgram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['โรตีทุเรียน',\n",
       "  'ตำร้อยเอ็ด',\n",
       "  'สันคอย่างแจ่ว',\n",
       "  'เกี๊ยวกุ้งทอดครีมสลัด',\n",
       "  'ส้มตำปูนึ่ง'],\n",
       " ['ก็ไม่ผิดหวัง',\n",
       "  'ที่จอดรถสะดวก',\n",
       "  'ให้เลือกหลายอย่าง',\n",
       "  'ถือว่าคุ้มค่า',\n",
       "  'กระพงทอดน้ำปลา'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample sorted list by the priority\n",
    "tokenize_dict[:5], tokenize_dict[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 782 ms, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = clean_special_char(train)\n",
    "train['tokenized'] = train['review'].apply(lambda x: word_tokenize(x,engine='new_mm',keep_whitespace=False, custom_dict=trie))\n",
    "train.to_pickle('newmm_tokenized_with_wongnai_topwords_customdict.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debug : tokenized by custom_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ร้านอาหารใหญ่มากกกกกกก  เลี้ยวเข้ามาเจอห้องน้ำ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>อาหารที่นี่เป็นอาหารจีนแคะที่หากินยากในบ้านเรา...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ปอเปี๊ยะสด ทุกวันนี้รู้สึกว่าหากินยาก  ร้านที่...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>รัานคัพเค้กในเมืองไทยมีไม่มาก หลาย คนอาจจะสงสั...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>อร่อย    เดินผ่าน               ทุกวัน ไม่ยักร...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  ร้านอาหารใหญ่มากกกกกกก  เลี้ยวเข้ามาเจอห้องน้ำ...\n",
       "1  อาหารที่นี่เป็นอาหารจีนแคะที่หากินยากในบ้านเรา...\n",
       "2  ปอเปี๊ยะสด ทุกวันนี้รู้สึกว่าหากินยาก  ร้านที่...\n",
       "3  รัานคัพเค้กในเมืองไทยมีไม่มาก หลาย คนอาจจะสงสั...\n",
       "4  อร่อย    เดินผ่าน               ทุกวัน ไม่ยักร..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import deepcut\n",
    "debug = clean_special_char(train[:5].copy())\n",
    "debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 7s, sys: 235 ms, total: 2min 7s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "debug_res = debug['review'].apply(lambda x: deepcut.tokenize(x, custom_dict=tokenize_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 6s, sys: 225 ms, total: 2min 7s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "debug['review_nospace'] = debug['review'].str.replace(' ', '')\n",
    "debug_res2 = debug['review_nospace'].apply(lambda x: deepcut.tokenize(x, custom_dict=tokenize_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it uses the same amount of time for the both library > so, choosing the pythainlp for the sake of flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ร้าน',\n",
       " 'อาหาร',\n",
       " 'ใหญ่มาก',\n",
       " 'กกกกกก',\n",
       " ' ',\n",
       " ' ',\n",
       " 'เลี้ยว',\n",
       " 'เข้า',\n",
       " 'มา',\n",
       " 'เจอ',\n",
       " 'ห้องน้ำ',\n",
       " 'ก่อน',\n",
       " 'เลย',\n",
       " ' ',\n",
       " 'เออแปลก',\n",
       " 'ดี',\n",
       " ' ',\n",
       " ' ',\n",
       " 'ห้อง',\n",
       " 'ทาน',\n",
       " 'หลัก',\n",
       " ' ',\n",
       " 'อยู่',\n",
       " 'ชั้น',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'มี',\n",
       " 'กาแฟ',\n",
       " ' ',\n",
       " 'น้ำผึ้ง',\n",
       " ' ',\n",
       " 'ซึ่ง',\n",
       " 'ก็',\n",
       " 'แค่',\n",
       " 'เอา',\n",
       " 'น้ำผึ้ง',\n",
       " 'มา',\n",
       " 'ราด',\n",
       " ' แพงเวอร์',\n",
       " ' ',\n",
       " 'อย่า',\n",
       " 'สั่งเลย',\n",
       " ' ',\n",
       " ' ',\n",
       " 'ลาบไข่ต้ม',\n",
       " ' ',\n",
       " 'ไข่',\n",
       " 'มัน',\n",
       " 'คาว',\n",
       " 'อะ',\n",
       " ' ',\n",
       " 'เลย',\n",
       " 'ไม่',\n",
       " 'ประทับใจ',\n",
       " 'เท่า',\n",
       " 'ไหร่',\n",
       " ' ',\n",
       " 'ทอดมันหัวปลี',\n",
       " 'กรอบ',\n",
       " 'อร่อย',\n",
       " 'ต้อง',\n",
       " 'เบิ้ล',\n",
       " ' ',\n",
       " ' ',\n",
       " 'พะแนงห่อไข่',\n",
       " 'อร่อยดี',\n",
       " ' ',\n",
       " 'เห้ยแต่',\n",
       " 'ราคา',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'บาท',\n",
       " 'มัน',\n",
       " 'เกิน',\n",
       " 'ไป',\n",
       " 'นะ',\n",
       " ' ',\n",
       " 'รับ',\n",
       " 'ไม่',\n",
       " 'ไหวว',\n",
       " ' ',\n",
       " 'เลิก',\n",
       " 'กิน',\n",
       " 'แล้ว',\n",
       " 'มี',\n",
       " 'ขนมหวาน',\n",
       " 'ให้',\n",
       " 'กิน',\n",
       " 'ฟรี',\n",
       " 'เล็กน้อย',\n",
       " ' ',\n",
       " ' ',\n",
       " 'ขนมไทย',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'คง',\n",
       " 'ไม่',\n",
       " 'ไป',\n",
       " 'ซ้ำ',\n",
       " ' ',\n",
       " 'แพงเกิน',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ร้าน',\n",
       " 'อาหาร',\n",
       " 'ใหญ่มาก',\n",
       " 'กกกกกก',\n",
       " 'เลี้ยว',\n",
       " 'เข้า',\n",
       " 'มา',\n",
       " 'เจอ',\n",
       " 'ห้องน้ำ',\n",
       " 'ก่อน',\n",
       " 'เลย',\n",
       " 'เออ',\n",
       " 'แปลก',\n",
       " 'ดี',\n",
       " 'ห้อง',\n",
       " 'ทาน',\n",
       " 'หลัก',\n",
       " 'อยู่',\n",
       " 'ชั้น',\n",
       " 'มี',\n",
       " 'กาแฟน้ำผึ้ง',\n",
       " 'ซึ่ง',\n",
       " 'ก็',\n",
       " 'แค่',\n",
       " 'เอา',\n",
       " 'น้ำผึ้ง',\n",
       " 'มา',\n",
       " 'ราด',\n",
       " 'แพงเวอร์',\n",
       " 'อย่า',\n",
       " 'สั่งเลย',\n",
       " 'ลาบไข่ต้ม',\n",
       " 'ไข่',\n",
       " 'มัน',\n",
       " 'คาว',\n",
       " 'อะเลย',\n",
       " 'ไม่',\n",
       " 'ประทับใจ',\n",
       " 'เท่า',\n",
       " 'ไหร่',\n",
       " 'ทอดมันหัวปลี',\n",
       " 'กรอบ',\n",
       " 'อร่อย',\n",
       " 'ต้อง',\n",
       " 'เบิ้ล',\n",
       " 'พะแนงห่อไข่',\n",
       " 'อร่อยดี',\n",
       " 'เห้ย',\n",
       " 'แต่',\n",
       " 'ราคา',\n",
       " 'บาท',\n",
       " 'มัน',\n",
       " 'เกิน',\n",
       " 'ไป',\n",
       " 'นะ',\n",
       " 'รับ',\n",
       " 'ไม่',\n",
       " 'ไหวว',\n",
       " 'เลิก',\n",
       " 'กิน',\n",
       " 'แล้ว',\n",
       " 'มี',\n",
       " 'ขนมหวาน',\n",
       " 'ให้',\n",
       " 'กิน',\n",
       " 'ฟรี',\n",
       " 'เล็กน้อย',\n",
       " 'ขนมไทย',\n",
       " 'คง',\n",
       " 'ไม่',\n",
       " 'ไป',\n",
       " 'ซ้ำ',\n",
       " 'แพง',\n",
       " 'เกิน']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_res2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key take aways here :\n",
    "1. the deep cut will process the cutting word from the endding of the list (so, placed the importance keyword at last!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_trie = pythainlp.tokenize.dict_trie(dict_source=tokenize_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.54 s, sys: 137 ms, total: 1.68 s\n",
      "Wall time: 7.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "debug['review_nospace'] = debug['review'].str.replace(' ', '')\n",
    "debug_res3 = debug['review_nospace'].apply(lambda x: pythainlp.word_tokenize(x,\n",
    "                                                                             engine='attacut',\n",
    "                                                                             keep_whitespace=False,\n",
    "                                                                             custom_dict=debug_trie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ร้าน',\n",
       " 'อาหาร',\n",
       " 'ใหญ่',\n",
       " 'มาก',\n",
       " 'กกกกกก',\n",
       " 'เลี้ยว',\n",
       " 'เข้า',\n",
       " 'มา',\n",
       " 'เจอ',\n",
       " 'ห้องน้ำ',\n",
       " 'ก่อน',\n",
       " 'เลย',\n",
       " 'เออ',\n",
       " 'แปลก',\n",
       " 'ดี',\n",
       " 'ห้อง',\n",
       " 'ทาน',\n",
       " 'หลัก',\n",
       " 'อยู่',\n",
       " 'ชั้น',\n",
       " 'มี',\n",
       " 'กาแฟ',\n",
       " 'น้ำ',\n",
       " 'ผึ้ง',\n",
       " 'ซึ่ง',\n",
       " 'ก็',\n",
       " 'แค่',\n",
       " 'เอา',\n",
       " 'น้ำ',\n",
       " 'ผึ้ง',\n",
       " 'มา',\n",
       " 'ราด',\n",
       " 'แพง',\n",
       " 'เวอร์',\n",
       " 'อย่า',\n",
       " 'สั่ง',\n",
       " 'เลย',\n",
       " 'ลาบไข่',\n",
       " 'ต้ม',\n",
       " 'ไข่',\n",
       " 'มัน',\n",
       " 'คาว',\n",
       " 'อะ',\n",
       " 'เลย',\n",
       " 'ไม่',\n",
       " 'ประทับใจ',\n",
       " 'เท่า',\n",
       " 'ไหร่',\n",
       " 'ทอด',\n",
       " 'มัน',\n",
       " 'หัว',\n",
       " 'ปลีก',\n",
       " 'รอบ',\n",
       " 'อร่อย',\n",
       " 'ต้อง',\n",
       " 'เบิ้ลพะแนง',\n",
       " 'ห่อ',\n",
       " 'ไข่',\n",
       " 'อร่อย',\n",
       " 'ดี',\n",
       " 'เห้ย',\n",
       " 'แต่',\n",
       " 'ราคา',\n",
       " 'บาท',\n",
       " 'มัน',\n",
       " 'เกิน',\n",
       " 'ไป',\n",
       " 'นะ',\n",
       " 'รับ',\n",
       " 'ไม่',\n",
       " 'ไหวว',\n",
       " 'เลิก',\n",
       " 'กิน',\n",
       " 'แล้ว',\n",
       " 'มี',\n",
       " 'ขนม',\n",
       " 'หวาน',\n",
       " 'ให้',\n",
       " 'กิน',\n",
       " 'ฟรี',\n",
       " 'เล็กน้อย',\n",
       " 'ขนม',\n",
       " 'ไทย',\n",
       " 'คง',\n",
       " 'ไม่',\n",
       " 'ไป',\n",
       " 'ซ้ำ',\n",
       " 'แพง',\n",
       " 'เกิน']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_res3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 ms, sys: 17 µs, total: 14.9 ms\n",
      "Wall time: 31.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "debug['review_nospace'] = debug['review'].str.replace(' ', '')\n",
    "debug_res4 = debug['review_nospace'].apply(lambda x: pythainlp.word_tokenize(x,\n",
    "                                                                             engine='new_mm',\n",
    "                                                                             keep_whitespace=False,\n",
    "                                                                             custom_dict=debug_trie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ร้านอาหาร',\n",
       " 'ใหญ่มาก',\n",
       " 'กกกกกกเลี้ยวเข้ามาเจอ',\n",
       " 'ห้องน้ำ',\n",
       " 'ก่อนเลยเออแปลกดีห้องทานหลักอยู่ชั้นมี',\n",
       " 'กาแฟน้ำผึ้ง',\n",
       " 'ซึ่งก็แค่เอา',\n",
       " 'น้ำผึ้ง',\n",
       " 'มาราดแพงเวอร์อย่า',\n",
       " 'สั่งเลย',\n",
       " 'ลาบไข่',\n",
       " 'ต้มไข่',\n",
       " 'มันคาวอะเลยไม่ประทับใจเท่าไหร่',\n",
       " 'ทอดมันหัวปลี',\n",
       " 'กรอบอร่อย',\n",
       " 'ต้องเบิ้ล',\n",
       " 'พะแนงห่อไข่',\n",
       " 'อร่อยดี',\n",
       " 'เห้ยแต่ราคาบาทมันเกินไปนะรับไม่ไหววเลิกกินแล้วมี',\n",
       " 'ขนมหวาน',\n",
       " 'ให้กินฟรีเล็กน้อย',\n",
       " 'ขนมไทย',\n",
       " 'คงไม่ไปซ้ำแพงเกิน']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_res4[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deepcut result seem very promising, so we need to make it parallelized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiprocess deepcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ร้านอาหารใหญ่มากกกกกกก \\nเลี้ยวเข้ามาเจอห้องน้...</td>\n",
       "      <td>050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>อาหารที่นี่เป็นอาหารจีนแคะที่หากินยากในบ้านเรา...</td>\n",
       "      <td>085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ปอเปี๊ยะสด ทุกวันนี้รู้สึกว่าหากินยาก (ร้านที่...</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>รัานคัพเค้กในเมืองไทยมีไม่มาก หลายๆคนอาจจะสงสั...</td>\n",
       "      <td>037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>อร่อย!!! เดินผ่านDigital gatewayทุกวัน ไม่ยักร...</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review chunk\n",
       "0  ร้านอาหารใหญ่มากกกกกกก \\nเลี้ยวเข้ามาเจอห้องน้...   050\n",
       "1  อาหารที่นี่เป็นอาหารจีนแคะที่หากินยากในบ้านเรา...   085\n",
       "2  ปอเปี๊ยะสด ทุกวันนี้รู้สึกว่าหากินยาก (ร้านที่...   137\n",
       "3  รัานคัพเค้กในเมืองไทยมีไม่มาก หลายๆคนอาจจะสงสั...   037\n",
       "4  อร่อย!!! เดินผ่านDigital gatewayทุกวัน ไม่ยักร...   193"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['chunk'] = np.random.randint(1,200 + 1,train.shape[0])\n",
    "train['chunk'] = train['chunk'].apply(lambda x : f\"{x:03}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'chunk=001'  'chunk=035'  'chunk=069'  'chunk=103'  'chunk=137'  'chunk=171'\r\n",
      "'chunk=002'  'chunk=036'  'chunk=070'  'chunk=104'  'chunk=138'  'chunk=172'\r\n",
      "'chunk=003'  'chunk=037'  'chunk=071'  'chunk=105'  'chunk=139'  'chunk=173'\r\n",
      "'chunk=004'  'chunk=038'  'chunk=072'  'chunk=106'  'chunk=140'  'chunk=174'\r\n",
      "'chunk=005'  'chunk=039'  'chunk=073'  'chunk=107'  'chunk=141'  'chunk=175'\r\n",
      "'chunk=006'  'chunk=040'  'chunk=074'  'chunk=108'  'chunk=142'  'chunk=176'\r\n",
      "'chunk=007'  'chunk=041'  'chunk=075'  'chunk=109'  'chunk=143'  'chunk=177'\r\n",
      "'chunk=008'  'chunk=042'  'chunk=076'  'chunk=110'  'chunk=144'  'chunk=178'\r\n",
      "'chunk=009'  'chunk=043'  'chunk=077'  'chunk=111'  'chunk=145'  'chunk=179'\r\n",
      "'chunk=010'  'chunk=044'  'chunk=078'  'chunk=112'  'chunk=146'  'chunk=180'\r\n",
      "'chunk=011'  'chunk=045'  'chunk=079'  'chunk=113'  'chunk=147'  'chunk=181'\r\n",
      "'chunk=012'  'chunk=046'  'chunk=080'  'chunk=114'  'chunk=148'  'chunk=182'\r\n",
      "'chunk=013'  'chunk=047'  'chunk=081'  'chunk=115'  'chunk=149'  'chunk=183'\r\n",
      "'chunk=014'  'chunk=048'  'chunk=082'  'chunk=116'  'chunk=150'  'chunk=184'\r\n",
      "'chunk=015'  'chunk=049'  'chunk=083'  'chunk=117'  'chunk=151'  'chunk=185'\r\n",
      "'chunk=016'  'chunk=050'  'chunk=084'  'chunk=118'  'chunk=152'  'chunk=186'\r\n",
      "'chunk=017'  'chunk=051'  'chunk=085'  'chunk=119'  'chunk=153'  'chunk=187'\r\n",
      "'chunk=018'  'chunk=052'  'chunk=086'  'chunk=120'  'chunk=154'  'chunk=188'\r\n",
      "'chunk=019'  'chunk=053'  'chunk=087'  'chunk=121'  'chunk=155'  'chunk=189'\r\n",
      "'chunk=020'  'chunk=054'  'chunk=088'  'chunk=122'  'chunk=156'  'chunk=190'\r\n",
      "'chunk=021'  'chunk=055'  'chunk=089'  'chunk=123'  'chunk=157'  'chunk=191'\r\n",
      "'chunk=022'  'chunk=056'  'chunk=090'  'chunk=124'  'chunk=158'  'chunk=192'\r\n",
      "'chunk=023'  'chunk=057'  'chunk=091'  'chunk=125'  'chunk=159'  'chunk=193'\r\n",
      "'chunk=024'  'chunk=058'  'chunk=092'  'chunk=126'  'chunk=160'  'chunk=194'\r\n",
      "'chunk=025'  'chunk=059'  'chunk=093'  'chunk=127'  'chunk=161'  'chunk=195'\r\n",
      "'chunk=026'  'chunk=060'  'chunk=094'  'chunk=128'  'chunk=162'  'chunk=196'\r\n",
      "'chunk=027'  'chunk=061'  'chunk=095'  'chunk=129'  'chunk=163'  'chunk=197'\r\n",
      "'chunk=028'  'chunk=062'  'chunk=096'  'chunk=130'  'chunk=164'  'chunk=198'\r\n",
      "'chunk=029'  'chunk=063'  'chunk=097'  'chunk=131'  'chunk=165'  'chunk=199'\r\n",
      "'chunk=030'  'chunk=064'  'chunk=098'  'chunk=132'  'chunk=166'  'chunk=200'\r\n",
      "'chunk=031'  'chunk=065'  'chunk=099'  'chunk=133'  'chunk=167'\r\n",
      "'chunk=032'  'chunk=066'  'chunk=100'  'chunk=134'  'chunk=168'\r\n",
      "'chunk=033'  'chunk=067'  'chunk=101'  'chunk=135'  'chunk=169'\r\n",
      "'chunk=034'  'chunk=068'  'chunk=102'  'chunk=136'  'chunk=170'\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/train_chunk/source/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_parquet(path='../data/train_chunk/source', partition_cols='chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "del deepcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_MAIN_FREE\"] = \"1\"\n",
    "os.system(f\"taskset -p 0xff {os.getpid()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chunk  review\n",
       "0   001     161\n",
       "1   002     186\n",
       "2   003     212\n",
       "3   004     198\n",
       "4   005     184"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_chunk_row = pd.read_parquet('../data/train_chunk/source/')\n",
    "num_chunk_row = num_chunk_row.groupby('chunk',as_index=False).count()\n",
    "num_chunk_row['chunk'] = num_chunk_row['chunk'].apply(lambda x: f\"{x:03}\")\n",
    "num_chunk_row.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_complete = list()\n",
    "for file in glob.glob('../data/train_chunk/result/dc_topword_food_dict/*'):\n",
    "    num_complete.append(file.split('/')[-1].replace('.parq',''))\n",
    "chunk_list = [f\"{x:03}\" for x in range(1,201)]\n",
    "to_process = set(chunk_list).difference(set(num_complete))\n",
    "to_process = list(to_process)\n",
    "to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob('../data/train_chunk/result/dc_topword_food_dict/*'):\n",
    "    num_ = file.split('/')[-1].replace('.parq','')\n",
    "    row_src = num_chunk_row.loc[num_chunk_row['chunk'] == num_, 'review'].values[0]\n",
    "    df_ = pd.read_parquet(file)\n",
    "    row_res = df_.shape[0]\n",
    "    assert row_res == row_src, f'the data for {num_} is not correct row_src {row_src} | row_res {row_res}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001.parq  008.parq  015.parq  022.parq\t029.parq  042.parq  052.parq  059.parq\r\n",
      "002.parq  009.parq  016.parq  023.parq\t030.parq  043.parq  053.parq  060.parq\r\n",
      "003.parq  010.parq  017.parq  024.parq\t031.parq  044.parq  054.parq  061.parq\r\n",
      "004.parq  011.parq  018.parq  025.parq\t032.parq  045.parq  055.parq\r\n",
      "005.parq  012.parq  019.parq  026.parq\t033.parq  049.parq  056.parq\r\n",
      "006.parq  013.parq  020.parq  027.parq\t037.parq  050.parq  057.parq\r\n",
      "007.parq  014.parq  021.parq  028.parq\t041.parq  051.parq  058.parq\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/train_chunk/result/dc_topword_food_dict/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read file from ../data/train_chunk/source/chunk=035\n",
      "read file from ../data/train_chunk/source/chunk=036\n",
      "train shape (207, 1) for ../data/train_chunk/source/chunk=035\n",
      "clean_char : 035\n",
      "read file from ../data/train_chunk/source/chunk=038\n",
      "train shape (203, 1) for ../data/train_chunk/source/chunk=036\n",
      "clean_char : 036\n",
      "train shape (203, 1) for ../data/train_chunk/source/chunk=038\n",
      "clean_char : 038\n",
      "read file from ../data/train_chunk/source/chunk=034\n",
      "tokenizing : 035\n",
      "tokenizing : 036\n",
      "tokenizing : 038\n",
      "train shape (198, 1) for ../data/train_chunk/source/chunk=034\n",
      "clean_char : 034\n",
      "read file from ../data/train_chunk/source/chunk=039\n",
      "read file from ../data/train_chunk/source/chunk=048\n",
      "train shape (204, 1) for ../data/train_chunk/source/chunk=039\n",
      "clean_char : 039\n",
      "tokenizing : 034\n",
      "train shape (182, 1) for ../data/train_chunk/source/chunk=048\n",
      "clean_char : 048\n",
      "tokenizing : 039\n",
      "tokenizing : 048\n",
      "read file from ../data/train_chunk/source/chunk=047\n",
      "read file from ../data/train_chunk/source/chunk=046\n",
      "train shape (189, 1) for ../data/train_chunk/source/chunk=047\n",
      "clean_char : 047\n",
      "train shape (217, 1) for ../data/train_chunk/source/chunk=046\n",
      "clean_char : 046\n",
      "tokenizing : 047\n",
      "tokenizing : 046\n",
      "read file from ../data/train_chunk/source/chunk=040\n",
      "train shape (192, 1) for ../data/train_chunk/source/chunk=040\n",
      "clean_char : 040\n",
      "tokenizing : 040\n",
      "write to ../data/train_chunk/result/dc_topword_food_dict/048.parq\n",
      "write to ../data/train_chunk/result/dc_topword_food_dict/047.parq\n",
      "write to ../data/train_chunk/result/dc_topword_food_dict/040.parq\n",
      "write to ../data/train_chunk/result/dc_topword_food_dict/034.parq\n",
      "write to ../data/train_chunk/result/dc_topword_food_dict/036.parq\n",
      "write to ../data/train_chunk/result/dc_topword_food_dict/038.parq\n",
      "write to ../data/train_chunk/result/dc_topword_food_dict/039.parq\n",
      "write to ../data/train_chunk/result/dc_topword_food_dict/035.parq\n",
      "write to ../data/train_chunk/result/dc_topword_food_dict/046.parq\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "\n",
    "def tokenize_process(num_chunk):\n",
    "    \n",
    "    import deepcut\n",
    "    import pandas as pd\n",
    "    import gc\n",
    "    \n",
    "    import os\n",
    "    import multiprocessing\n",
    "    pool_size = multiprocessing.cpu_count()\n",
    "    os.environ[\"OPENBLAS_MAIN_FREE\"] = \"1\"\n",
    "    os.system('taskset -cp 0-%d %s' % (pool_size, os.getpid()))\n",
    "    \n",
    "    def tokenize(s):\n",
    "        import deepcut\n",
    "        return deepcut.tokenize(s, custom_dict=tokenize_dict)\n",
    "    \n",
    "    def clean_special_char(train):\n",
    "\n",
    "        special_char = r\"[\\\\\\@\\^\\%\\(\\*\\|\\<\\`\\.\\_\\=\\!\\>\\;\\?\\#\\$\\)\\/\\,\\ๆ\\'\\~\\:\\+\\-\\\"\\฿]\"\n",
    "        space_char = r\"[\\n\\&]\"\n",
    "        num = '[0-9]'\n",
    "        num_th = '[๑๒๓๓๔๕๖๗๘๙]'\n",
    "        eng_words = '[A-Za-z]'\n",
    "\n",
    "        train['review'] = train['review'].str.replace(special_char, ' ')\n",
    "        train['review'] = train['review'].str.replace(space_char, ' ')\n",
    "        train['review'] = train['review'].str.replace(num, ' ', regex=True)\n",
    "        train['review'] = train['review'].str.replace(num_th, ' ', regex=True)\n",
    "        train['review'] = train['review'].str.replace(eng_words, ' ', regex=True)\n",
    "        train['review'] = train['review'].str.lower()\n",
    "\n",
    "        return train\n",
    "    \n",
    "    print(f'read file from ../data/train_chunk/source/chunk={num_chunk}')\n",
    "    train = pd.read_parquet(f'../data/train_chunk/source/chunk={num_chunk}')\n",
    "    print(f'train shape {train.shape} for ../data/train_chunk/source/chunk={num_chunk}')\n",
    "    \n",
    "    print(f'clean_char : {num_chunk}')\n",
    "    train = clean_special_char(train)\n",
    "    train['review_nospace'] = train['review'].str.replace(' ', '')\n",
    "    \n",
    "    print(f'tokenizing : {num_chunk}')\n",
    "    train['tokenized'] = train['review_nospace'].apply(tokenize)\n",
    "    \n",
    "    print(f'write to ../data/train_chunk/result/dc_topword_food_dict/{num_chunk}.parq')\n",
    "    train.to_parquet(f'../data/train_chunk/result/dc_topword_food_dict/{num_chunk}.parq')\n",
    "    \n",
    "    del train\n",
    "    gc.collect()\n",
    "    return True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(9) as P:\n",
    "#         chunk_list = [f\"{x:03}\" for x in range(62,201)]\n",
    "        P.map(func = tokenize_process, iterable = to_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key takeaways here:\n",
    "    1. paralellizing deepcut is hard...\n",
    "    2. https://shahhj.wordpress.com/2013/10/27/numpy-and-blas-no-problemo/ (make numpy, padnas not strict to single core)\n",
    "    3. https://stackoverflow.com/questions/23537716/importing-scipy-breaks-multiprocessing-support-in-python/23546547\n",
    "    4. https://stackoverflow.com/questions/15639779/why-does-multiprocessing-use-only-a-single-core-after-i-import-numpy\n",
    "    5. https://stackoverflow.com/questions/50306632/multiprocessing-not-achieving-full-cpu-usage-on-dual-processor-windows-machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# consolidate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('../data/train_chunk/result/dc_topword_food_dict/')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_top_freq_word(series, n):\n",
    "    all_word = [a for b in series.tolist() for a in b]\n",
    "    Counter_ = Counter(all_word)\n",
    "    most_occur = Counter_.most_common(n) \n",
    "    return most_occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keep_words = ['อร่อย','รสชาติ','ราคา','หวาน','ชอบ','เนื้อ','บริการ','ใหญ่','บรรยากาศ',\n",
    "              'แนะนำ','พนักงาน','แพง','นาน','อร่อยมาก','น้ำจิ้ม','เครื่องดื่ม','น้ำซุป','เข้มข้น',\n",
    "              'กาแฟ','มาลอง','อร่อยดี','ธรรมดา','ปกติ','ขนมปัง','ญี่ปุ่น','ชาเขียว','แปลก','ใช้ได้','⭐',\n",
    "              'บรรยากาศร้าน','ก๋วยเตี๋ยว','หน้าตา','ครอบครัว','ราคาถูก','ปรับปรุง','สถานที่','ไส้กรอก','กลับบ้าน',\n",
    "             'เป็นร้านเล็ก','พอใช้ได้']\n",
    "top_remove_words = [w[0] for w in get_top_freq_word(df['tokenized'], n=1000) if w[0] not in keep_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tok_remove_topwords'] = df['tokenized'].apply(lambda x : [w for w in x if w not in top_remove_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ราคา', 19677),\n",
       " ('รสชาติ', 18713),\n",
       " ('ชอบ', 13584),\n",
       " ('อร่อย', 12611),\n",
       " ('หวาน', 8368),\n",
       " ('บริการ', 7980),\n",
       " ('เนื้อ', 6944),\n",
       " ('ใหญ่', 6921),\n",
       " ('แนะนำ', 6893),\n",
       " ('พนักงาน', 6750),\n",
       " ('บรรยากาศ', 5757),\n",
       " ('อร่อยมาก', 4389),\n",
       " ('น้ำจิ้ม', 4257),\n",
       " ('เครื่องดื่ม', 4033),\n",
       " ('น้ำซุป', 3954),\n",
       " ('เข้มข้น', 3806),\n",
       " ('กาแฟ', 3652),\n",
       " ('ธรรมดา', 3539),\n",
       " ('นาน', 3383),\n",
       " ('มาลอง', 3251),\n",
       " ('อร่อยดี', 3218),\n",
       " ('ปกติ', 3030),\n",
       " ('แพง', 2324),\n",
       " ('ขนมปัง', 2147),\n",
       " ('ญี่ปุ่น', 2147),\n",
       " ('ชาเขียว', 2083),\n",
       " ('แปลก', 2069),\n",
       " ('ใช้ได้', 2054),\n",
       " ('⭐', 2037),\n",
       " ('บรรยากาศร้าน', 2005),\n",
       " ('ก๋วยเตี๋ยว', 1989),\n",
       " ('หน้าตา', 1165),\n",
       " ('ครอบครัว', 1159),\n",
       " ('ราคาถูก', 747),\n",
       " ('ปรับปรุง', 746),\n",
       " ('สถานที่', 744),\n",
       " ('ไส้กรอก', 739),\n",
       " ('กลับบ้าน', 736),\n",
       " ('เป็นร้านเล็ก', 736),\n",
       " ('พอใช้ได้', 733),\n",
       " ('ซ่า', 448),\n",
       " ('เป็นต้น', 448),\n",
       " ('โบราณ', 446),\n",
       " ('ซีฟู้ด', 446),\n",
       " ('หอมมาก', 445),\n",
       " ('เทศกาล', 445),\n",
       " ('ไม่ควรพลาด', 444),\n",
       " ('อาคาร', 443),\n",
       " ('ตบท้าย', 443),\n",
       " ('ทานที่ร้าน', 441)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_freq_word(df['tok_remove_topwords'], n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    40000.000000\n",
       "mean        24.088325\n",
       "std         23.829494\n",
       "min          0.000000\n",
       "25%         12.000000\n",
       "50%         17.000000\n",
       "75%         28.000000\n",
       "max       1187.000000\n",
       "Name: tok_remove_topwords, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tok_remove_topwords'].apply(lambda x:len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    40000.000000\n",
       "mean       111.989425\n",
       "std        112.933507\n",
       "min          1.000000\n",
       "25%         53.000000\n",
       "50%         77.000000\n",
       "75%        127.000000\n",
       "max       4477.000000\n",
       "Name: tokenized, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized'].apply(lambda x:len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_parquet('../data/train_chunk/result/dc_topword_food_dict_remove_single_words_1K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.78 s, sys: 204 ms, total: 1.98 s\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "# bigram\n",
    "# input_ = train['remove_top100_stopwords'].tolist()\n",
    "# bigram = gensim.models.Phrases(input_, min_count=10, threshold=10) # higher threshold fewer phrases.\n",
    "# bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# data_words_bigrams = make_bigrams(input_)\n",
    "# train['bigrams'] = data_words_bigrams\n",
    "\n",
    "# bigram\n",
    "\n",
    "train['bigrams'] = train['remove_top100_stopwords'].apply(lambda row: list(nltk.ngrams(row, 2)))\n",
    "train['bigrams'] = train['bigrams'].apply(lambda row: ['_'.join(gram) for gram in row])\n",
    "\n",
    "# trigram\n",
    "# trigram = gensim.models.Phrases(bigram[data_words], threshold=10)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "# data_words_trigrams = make_trigrams(data_words_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['ง่าย_ใจกลาง', 'ใจกลาง_เมือง', 'เมือง_แหนม', 'แหนม_เนืองรสชาติ', 'เนืองรสชาติ_ซื้อ', 'ซื้อ_ฝาก', 'ฝาก_ปริมาณ', 'ปริมาณ_กะ', 'กะ_ลำบาก', 'ลำบาก_บาง', 'บาง_ควร', 'ควร_น้อย', 'น้อย_ควร', 'ควร_น้อย', 'น้อย_แนะนำ', 'แนะนำ_ถาม', 'ถาม_น้อง', 'น้อง_พนักงาน', 'พนักงาน_ที', 'ที_ปริมาณ', 'ปริมาณ_เหมาะ', 'เหมาะ_กี่', 'กี่_ทยอย', 'ทยอย_ไม่', 'ไม่_เหลือ'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['bigrams'].sample(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('อร่อย_ดี', 6293),\n",
       " ('ไม่_แพง', 4997),\n",
       " ('ราคา_ไม่', 4935),\n",
       " ('กำลัง_ดี', 3744),\n",
       " ('ดี_ไม่', 3647),\n",
       " ('บริการ_ดี', 3613),\n",
       " ('ของ_หวาน', 2946),\n",
       " ('อร่อย_ไม่', 2672),\n",
       " ('รสชาติ_อร่อย', 2646),\n",
       " ('ไม่_อร่อย', 2607),\n",
       " ('รสชาติ_ดี', 2500),\n",
       " ('ไม่_รู้', 2411),\n",
       " ('ไม่_หวาน', 2280),\n",
       " ('ผิด_หวัง', 2230),\n",
       " ('ไม่_ชอบ', 2223),\n",
       " ('บรรยากาศ_ดี', 2126),\n",
       " ('ไม่_เคย', 2125),\n",
       " ('รสชาติ_ไม่', 2014),\n",
       " ('ไม่_ใช่', 1948),\n",
       " ('พนักงาน_บริการ', 1798),\n",
       " ('กลับ_บ้าน', 1747),\n",
       " ('ไม่_นาน', 1725),\n",
       " ('ไม่_ผิด', 1704),\n",
       " ('คุ้ม_ค่า', 1691),\n",
       " ('ไม่_ไม่', 1561),\n",
       " ('ถ่าย_รูป', 1559),\n",
       " ('หมูก_รอบ', 1478),\n",
       " ('ไม่_ถูก', 1375),\n",
       " ('⭐_️', 1318),\n",
       " ('ริม_ถนน', 1268)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_freq_word(train['bigrams'], n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T05:56:28.532277Z",
     "start_time": "2020-03-22T05:56:28.521260Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_word(df, stop_words = None, trie = None):\n",
    "    \n",
    "    # tokenized\n",
    "    if trie is None:\n",
    "        df['word_tokenized'] = df['review'].apply(lambda x: word_tokenize(x,engine='attacut',keep_whitespace=False))\n",
    "        df['len_review'] = df['word_tokenized'].apply(lambda x: len(x))\n",
    "        print('finish tokenize word')\n",
    "    else:\n",
    "        df['word_tokenized'] = df['review'].apply(lambda x: word_tokenize(x,engine='attacut',keep_whitespace=False,custom_dict=trie))\n",
    "        df['len_review'] = df['word_tokenized'].apply(lambda x: len(x))\n",
    "        print('finish tokenize word with custom_dict')\n",
    "\n",
    "    # remove duplicated word\n",
    "    df['word_tokenized_uniq'] = df['word_tokenized'].apply(lambda x: list(set(x)))\n",
    "    df['len_review_uniq'] = df['word_tokenized_uniq'].apply(lambda x: len(x))\n",
    "    print('finish remove duplicated word')\n",
    "\n",
    "    # remove stopword\n",
    "    thai_stopword_ls = list(pythainlp.corpus.thai_stopwords())\n",
    "    df['word_tokenized_uniq_cls_revstopw'] = df['word_tokenized_uniq'].apply(lambda x : [w for w in x if w not in thai_stopword_ls])\n",
    "    df['len_review_uniq_cls_revstopw'] = df['word_tokenized_uniq_cls_revstopw'].apply(lambda x: len(x))\n",
    "    print('finish remove basic stopwords')\n",
    "    \n",
    "    if stop_words is not None:\n",
    "        df['word_tokenized_uniq_cls_revstopw'] = df['word_tokenized_uniq_cls_revstopw'].apply(lambda x : [w for w in x if w not in stop_words])\n",
    "        df['len_review_uniq_cls_revstopw'] = df['word_tokenized_uniq_cls_revstopw'].apply(lambda x: len(x))\n",
    "        print('finish remove additional stopwords')\n",
    "\n",
    "    # describe stat\n",
    "    display(df[['len_review','len_review_uniq','len_review_uniq_cls_revstopw']].describe().transpose())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T05:57:02.520122Z",
     "start_time": "2020-03-22T05:56:28.535225Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish tokenize word with custom_dict\n",
      "finish remove duplicated word\n",
      "finish remove basic stopwords\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>len_review</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>141.8193</td>\n",
       "      <td>122.378546</td>\n",
       "      <td>10.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>166.25</td>\n",
       "      <td>1646.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>len_review_uniq</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>89.4904</td>\n",
       "      <td>51.873278</td>\n",
       "      <td>4.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>108.00</td>\n",
       "      <td>511.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>len_review_uniq_cls_revstopw</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>51.0161</td>\n",
       "      <td>33.320343</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>61.00</td>\n",
       "      <td>352.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                count      mean         std   min   25%  \\\n",
       "len_review                    10000.0  141.8193  122.378546  10.0  67.0   \n",
       "len_review_uniq               10000.0   89.4904   51.873278   4.0  53.0   \n",
       "len_review_uniq_cls_revstopw  10000.0   51.0161   33.320343   3.0  29.0   \n",
       "\n",
       "                                50%     75%     max  \n",
       "len_review                    104.0  166.25  1646.0  \n",
       "len_review_uniq                75.0  108.00   511.0  \n",
       "len_review_uniq_cls_revstopw   41.0   61.00   352.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_clean = tokenize_word(train_clean, trie=trie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create bigrams/trigrams/quardgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T05:57:02.530094Z",
     "start_time": "2020-03-22T05:57:02.522077Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_top_freq_word(series, n):\n",
    "    all_word = [a for b in series.tolist() for a in b]\n",
    "    Counter_ = Counter(all_word)\n",
    "    most_occur = Counter_.most_common(n) \n",
    "    return most_occur\n",
    "\n",
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "train['bigrams'] = train['word_tokenized'].apply(lambda row: list(nltk.ngrams(row, 2)))\n",
    "\n",
    "def create_n_grams(train):\n",
    "    \n",
    "    train['bigrams'] = train['word_tokenized'].apply(lambda row: list(nltk.ngrams(row, 2)))\n",
    "    train['trigrams'] = train['word_tokenized'].apply(lambda row: list(nltk.ngrams(row, 3)))\n",
    "    train['quardgrams'] = train['word_tokenized'].apply(lambda row: list(nltk.ngrams(row, 4)))\n",
    "    \n",
    "    train['bigrams'] = train['bigrams'].apply(lambda row: ['_'.join(gram) for gram in row])\n",
    "    train['trigrams'] = train['trigrams'].apply(lambda row: ['_'.join(gram) for gram in row])\n",
    "    train['quardgrams'] = train['quardgrams'].apply(lambda row: ['_'.join(gram) for gram in row])\n",
    "    train['bi_tri_grams'] = train['bigrams'] + train['trigrams']\n",
    "    train['all_grams'] = train['word_tokenized'] + train['bigrams'] + train['trigrams'] + train['quardgrams']\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T05:57:03.668689Z",
     "start_time": "2020-03-22T05:57:02.532094Z"
    }
   },
   "outputs": [],
   "source": [
    "train_clean = create_n_grams(train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T05:57:03.703609Z",
     "start_time": "2020-03-22T05:57:03.670720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_tokenized</th>\n",
       "      <th>len_review</th>\n",
       "      <th>word_tokenized_uniq</th>\n",
       "      <th>len_review_uniq</th>\n",
       "      <th>word_tokenized_uniq_cls_revstopw</th>\n",
       "      <th>len_review_uniq_cls_revstopw</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>quardgrams</th>\n",
       "      <th>bi_tri_grams</th>\n",
       "      <th>all_grams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ร้านอาหารใหญ่มากกกกกกก  เลี้ยวเข้ามาเจอห้องน้ำ...</td>\n",
       "      <td>[ร้าน, อาหาร, ใหญ่, มาก, กกกกกก, เลี้ยว, เข้า,...</td>\n",
       "      <td>89</td>\n",
       "      <td>[นะ, ร้าน, ต้อง, มาก, ซึ่ง, ผึ้ง, พะแนง, เล็กน...</td>\n",
       "      <td>73</td>\n",
       "      <td>[ร้าน, ผึ้ง, พะแนง, ต้ม, น้ำ, ไทย, เจอ, แปลก, ...</td>\n",
       "      <td>48</td>\n",
       "      <td>[ร้าน_อาหาร, อาหาร_ใหญ่, ใหญ่_มาก, มาก_กกกกกก,...</td>\n",
       "      <td>[ร้าน_อาหาร_ใหญ่, อาหาร_ใหญ่_มาก, ใหญ่_มาก_กกก...</td>\n",
       "      <td>[ร้าน_อาหาร_ใหญ่_มาก, อาหาร_ใหญ่_มาก_กกกกกก, ใ...</td>\n",
       "      <td>[ร้าน_อาหาร, อาหาร_ใหญ่, ใหญ่_มาก, มาก_กกกกกก,...</td>\n",
       "      <td>[ร้าน, อาหาร, ใหญ่, มาก, กกกกกก, เลี้ยว, เข้า,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  ร้านอาหารใหญ่มากกกกกกก  เลี้ยวเข้ามาเจอห้องน้ำ...   \n",
       "\n",
       "                                      word_tokenized  len_review  \\\n",
       "0  [ร้าน, อาหาร, ใหญ่, มาก, กกกกกก, เลี้ยว, เข้า,...          89   \n",
       "\n",
       "                                 word_tokenized_uniq  len_review_uniq  \\\n",
       "0  [นะ, ร้าน, ต้อง, มาก, ซึ่ง, ผึ้ง, พะแนง, เล็กน...               73   \n",
       "\n",
       "                    word_tokenized_uniq_cls_revstopw  \\\n",
       "0  [ร้าน, ผึ้ง, พะแนง, ต้ม, น้ำ, ไทย, เจอ, แปลก, ...   \n",
       "\n",
       "   len_review_uniq_cls_revstopw  \\\n",
       "0                            48   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [ร้าน_อาหาร, อาหาร_ใหญ่, ใหญ่_มาก, มาก_กกกกกก,...   \n",
       "\n",
       "                                            trigrams  \\\n",
       "0  [ร้าน_อาหาร_ใหญ่, อาหาร_ใหญ่_มาก, ใหญ่_มาก_กกก...   \n",
       "\n",
       "                                          quardgrams  \\\n",
       "0  [ร้าน_อาหาร_ใหญ่_มาก, อาหาร_ใหญ่_มาก_กกกกกก, ใ...   \n",
       "\n",
       "                                        bi_tri_grams  \\\n",
       "0  [ร้าน_อาหาร, อาหาร_ใหญ่, ใหญ่_มาก, มาก_กกกกกก,...   \n",
       "\n",
       "                                           all_grams  \n",
       "0  [ร้าน, อาหาร, ใหญ่, มาก, กกกกกก, เลี้ยว, เข้า,...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T06:00:40.687225Z",
     "start_time": "2020-03-22T06:00:40.356142Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words = get_top_freq_word(train_clean['bigrams'], 50)\n",
    "top_words_to_clean = [c[0] for c in top_words if c[0] not in ['อร่อย_มาก','ไม่_แพง','ส่วน_ตัว','ใช้_ได้','อร่อย_ดี']]\n",
    "train_clean['bi_tri_grams'] = train_clean['bi_tri_grams'].apply(lambda row: [c for c in row if c not in top_words_to_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T06:02:54.112319Z",
     "start_time": "2020-03-22T06:02:53.761818Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words = get_top_freq_word(train_clean['trigrams'], 50)\n",
    "top_words_to_clean = [c[0] for c in top_words if c[0] not in ['ราคา_ไม่_แพง','ใช้_ได้_เลย','ไม่_ผิด_หวัง','อร่อย_ใช้_ได้']]\n",
    "train_clean['bi_tri_grams'] = train_clean['bi_tri_grams'].apply(lambda row: [c for c in row if c not in top_words_to_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T06:02:59.571898Z",
     "start_time": "2020-03-22T06:02:59.567908Z"
    }
   },
   "outputs": [],
   "source": [
    "# output\n",
    "data_words = train['bi_tri_grams'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T06:03:04.560334Z",
     "start_time": "2020-03-22T06:03:04.450459Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean       271.470500\n",
       "std        237.836188\n",
       "min         14.000000\n",
       "25%        126.000000\n",
       "50%        198.000000\n",
       "75%        319.000000\n",
       "max       3198.000000\n",
       "Name: bi_tri_grams, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['bi_tri_grams'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T06:03:08.041990Z",
     "start_time": "2020-03-22T06:03:08.038996Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    " bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=10, threshold=20) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=10)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# add bigram trigram\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "data_words_trigrams = make_trigrams(data_words_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T16:00:36.115519Z",
     "start_time": "2020-03-14T16:00:36.110583Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "thai_stopword_ls = list(pythainlp.corpus.thai_stopwords())\n",
    "train['tokenized'] = train['word_tokenized'].apply(lambda x : [w for w in x if w not in thai_stopword_ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = train['tokenized'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T16:00:54.417243Z",
     "start_time": "2020-03-14T16:00:36.117514Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.8 s ± 27.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=10, threshold=20) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=10)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# add bigram trigram\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "data_words_trigrams = make_trigrams(data_words_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bigrams2'] = data_words_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ปอเปี๊ยะ_สด',\n",
       " 'สด_ทุก_ทุก_วัน',\n",
       " 'นี้_รู้สึก_รู้สึก_ว่า',\n",
       " 'ว่า_หากิน',\n",
       " 'หากิน_ยาก',\n",
       " 'ยาก_ร้าน',\n",
       " 'ที่_ขาย',\n",
       " 'ขาย_ปอเปี๊ยะ',\n",
       " 'ปอเปี๊ยะ_สด',\n",
       " 'สด_อย่าง',\n",
       " 'อย่าง_เดียว',\n",
       " 'เดียว_ส่วน',\n",
       " 'ส่วน_มาก_มาก_ที่',\n",
       " 'ที่_เจอ',\n",
       " 'เจอ_คือ',\n",
       " 'คือ_ขาย',\n",
       " 'ขาย_ตาม',\n",
       " 'ตาม_ภัตตาคาร',\n",
       " 'ภัตตาคาร_ซึ่ง',\n",
       " 'ซึ่ง_มัก',\n",
       " 'มัก_จะ_จะ_ไม่',\n",
       " 'ค่อย_อร่อย',\n",
       " 'อร่อย_ร้าน',\n",
       " 'นี้_ที่',\n",
       " 'ที่_สะพานเหลือง',\n",
       " 'สะพานเหลือง_ราคา',\n",
       " 'ราคา_ย่อมเยา',\n",
       " 'ย่อมเยา_เพียง',\n",
       " 'เพียง_แค่_แค่_บาท',\n",
       " 'บาท_จึง',\n",
       " 'จึง_ซื้อ',\n",
       " 'ซื้อ_กิน',\n",
       " 'กิน_ทุก_ทุก_ครั้ง',\n",
       " 'ครั้ง_ที่_ที่_มา',\n",
       " 'มา_รสชาติ_รสชาติ_ดี',\n",
       " 'ดี_พอสมควร',\n",
       " 'พอสมควร_เลย',\n",
       " 'เลย_โดย_โดย_เฉพาะ',\n",
       " 'เฉพาะ_เต้าหู้',\n",
       " 'เต้าหู้_กับ',\n",
       " 'กับ_กุนเชียง',\n",
       " 'กุนเชียง_ที่',\n",
       " 'ที่_ใส่_ใส่_มา',\n",
       " 'มา_ให้_ให้_อย่าง',\n",
       " 'อย่าง_จุใจ',\n",
       " 'จุใจ_ราด',\n",
       " 'ราด_น้ำ',\n",
       " 'จิ้ม_ที่',\n",
       " 'ที่_โอเค',\n",
       " 'โอเค_ไม่_ไม่_ถึง',\n",
       " 'ถึง_กับ',\n",
       " 'กับ_สุด',\n",
       " 'สุด_ยอด',\n",
       " 'ยอด_เสิร์ฟ',\n",
       " 'เสิร์ฟ_พรัอม',\n",
       " 'พรัอม_เครื่อง',\n",
       " 'เครื่อง_เคียง',\n",
       " 'เคียง_ต้น',\n",
       " 'ต้น_หอม',\n",
       " 'หอม_กับ',\n",
       " 'กับ_พริก',\n",
       " 'พริก_เขียว',\n",
       " 'เขียว_มา',\n",
       " 'มา_เสิร์ฟ_เสิร์ฟ_ให้',\n",
       " 'ให้_ถึง',\n",
       " 'ถึง_โต๊ะ',\n",
       " 'โต๊ะ_นั่ง',\n",
       " 'นั่ง_กิน',\n",
       " 'กิน_ร้าน',\n",
       " 'ร้าน_ข้าง',\n",
       " 'ข้าง_ครบถ้วน',\n",
       " 'ครบถ้วน_ได้',\n",
       " 'ได้_อารมณ์',\n",
       " 'อารมณ์_เอา',\n",
       " 'เอา_ไป_ไป_ดาว',\n",
       " 'ดาว_ครับ',\n",
       " 'ครับ_คุณ',\n",
       " 'คุณ_ป้า',\n",
       " 'ป้า_สำหรับ',\n",
       " 'สำหรับ_อาหาร',\n",
       " 'อาหาร_แต่',\n",
       " 'แต่_วัน_วัน_นั้น',\n",
       " 'นั้น_เจอ',\n",
       " 'เจอ_เหตุการณ์',\n",
       " 'เหตุการณ์_คุณ',\n",
       " 'คุณ_ป้า',\n",
       " 'ป้า_ให้',\n",
       " 'ให้_อาหาร',\n",
       " 'อาหาร_แมว',\n",
       " 'แมว_ระหว่าง',\n",
       " 'ระหว่าง_ทำ',\n",
       " 'ทำ_ปอเปี๊ยะ',\n",
       " 'ปอเปี๊ยะ_ไม่',\n",
       " 'ไม่_ใส่',\n",
       " 'ใส่_ถุง',\n",
       " 'ถุง_มือ',\n",
       " 'มือ_และ',\n",
       " 'และ_ไม่',\n",
       " 'ไม่_เห็น_เห็น_ว่า',\n",
       " 'ว่า_ล้าง',\n",
       " 'ล้าง_มือ',\n",
       " 'มือ_ผม',\n",
       " 'ผม_เลย',\n",
       " 'เลย_จำเป็น',\n",
       " 'จำเป็น_ต้อง',\n",
       " 'ต้อง_หัก',\n",
       " 'หัก_ออก',\n",
       " 'ออก_อีก',\n",
       " 'อีก_หนึ่ง',\n",
       " 'หนึ่ง_ดาว',\n",
       " 'ปอเปี๊ยะ_สด_ทุก',\n",
       " 'สด_ทุก_วัน',\n",
       " 'ทุก_วัน_นี้',\n",
       " 'วัน_นี้_รู้สึก',\n",
       " 'นี้_รู้สึก_ว่า',\n",
       " 'รู้สึก_ว่า_หากิน',\n",
       " 'ว่า_หากิน_ยาก',\n",
       " 'หากิน_ยาก_ร้าน',\n",
       " 'ยาก_ร้าน_ที่',\n",
       " 'ร้าน_ที่_ขาย',\n",
       " 'ที่_ขาย_ปอเปี๊ยะ',\n",
       " 'ขาย_ปอเปี๊ยะ_สด',\n",
       " 'ปอเปี๊ยะ_สด_อย่าง',\n",
       " 'สด_อย่าง_เดียว',\n",
       " 'อย่าง_เดียว_ส่วน',\n",
       " 'เดียว_ส่วน_มาก',\n",
       " 'ส่วน_มาก_ที่',\n",
       " 'มาก_ที่_เจอ',\n",
       " 'ที่_เจอ_คือ',\n",
       " 'เจอ_คือ_ขาย',\n",
       " 'คือ_ขาย_ตาม',\n",
       " 'ขาย_ตาม_ภัตตาคาร',\n",
       " 'ตาม_ภัตตาคาร_ซึ่ง',\n",
       " 'ภัตตาคาร_ซึ่ง_มัก',\n",
       " 'ซึ่ง_มัก_จะ',\n",
       " 'มัก_จะ_ไม่_จะ_ไม่_ค่อย',\n",
       " 'ไม่_ค่อย_อร่อย',\n",
       " 'ค่อย_อร่อย_ร้าน',\n",
       " 'อร่อย_ร้าน_นี้',\n",
       " 'ร้าน_นี้_ที่',\n",
       " 'นี้_ที่_สะพานเหลือง',\n",
       " 'ที่_สะพานเหลือง_ราคา',\n",
       " 'สะพานเหลือง_ราคา_ย่อมเยา',\n",
       " 'ราคา_ย่อมเยา_เพียง',\n",
       " 'ย่อมเยา_เพียง_แค่',\n",
       " 'เพียง_แค่_บาท',\n",
       " 'แค่_บาท_จึง',\n",
       " 'บาท_จึง_ซื้อ',\n",
       " 'จึง_ซื้อ_กิน',\n",
       " 'ซื้อ_กิน_ทุก',\n",
       " 'กิน_ทุก_ครั้ง_ทุก_ครั้ง_ที่',\n",
       " 'ครั้ง_ที่_มา',\n",
       " 'ที่_มา_รสชาติ',\n",
       " 'มา_รสชาติ_ดี',\n",
       " 'รสชาติ_ดี_พอสมควร',\n",
       " 'ดี_พอสมควร_เลย',\n",
       " 'พอสมควร_เลย_โดย',\n",
       " 'เลย_โดย_เฉพาะ',\n",
       " 'โดย_เฉพาะ_เต้าหู้',\n",
       " 'เฉพาะ_เต้าหู้_กับ',\n",
       " 'เต้าหู้_กับ_กุนเชียง',\n",
       " 'กับ_กุนเชียง_ที่',\n",
       " 'กุนเชียง_ที่_ใส่',\n",
       " 'ที่_ใส่_มา_ใส่_มา_ให้',\n",
       " 'มา_ให้_อย่าง',\n",
       " 'ให้_อย่าง_จุใจ',\n",
       " 'อย่าง_จุใจ_ราด',\n",
       " 'จุใจ_ราด_น้ำ',\n",
       " 'ราด_น้ำ_จิ้ม',\n",
       " 'น้ำ_จิ้ม_ที่',\n",
       " 'จิ้ม_ที่_โอเค',\n",
       " 'ที่_โอเค_ไม่',\n",
       " 'โอเค_ไม่_ถึง_ไม่_ถึง_กับ',\n",
       " 'ถึง_กับ_สุด',\n",
       " 'กับ_สุด_ยอด',\n",
       " 'สุด_ยอด_เสิร์ฟ',\n",
       " 'ยอด_เสิร์ฟ_พรัอม',\n",
       " 'เสิร์ฟ_พรัอม_เครื่อง',\n",
       " 'พรัอม_เครื่อง_เคียง',\n",
       " 'เครื่อง_เคียง_ต้น',\n",
       " 'เคียง_ต้น_หอม',\n",
       " 'ต้น_หอม_กับ',\n",
       " 'หอม_กับ_พริก',\n",
       " 'กับ_พริก_เขียว',\n",
       " 'พริก_เขียว_มา',\n",
       " 'เขียว_มา_เสิร์ฟ',\n",
       " 'มา_เสิร์ฟ_ให้',\n",
       " 'เสิร์ฟ_ให้_ถึง',\n",
       " 'ให้_ถึง_โต๊ะ',\n",
       " 'ถึง_โต๊ะ_นั่ง',\n",
       " 'โต๊ะ_นั่ง_กิน',\n",
       " 'นั่ง_กิน_ร้าน',\n",
       " 'กิน_ร้าน_ข้าง',\n",
       " 'ร้าน_ข้าง_ครบถ้วน',\n",
       " 'ข้าง_ครบถ้วน_ได้',\n",
       " 'ครบถ้วน_ได้_อารมณ์',\n",
       " 'ได้_อารมณ์_เอา',\n",
       " 'อารมณ์_เอา_ไป',\n",
       " 'เอา_ไป_ดาว',\n",
       " 'ไป_ดาว_ครับ',\n",
       " 'ดาว_ครับ_คุณ',\n",
       " 'ครับ_คุณ_ป้า',\n",
       " 'คุณ_ป้า_สำหรับ',\n",
       " 'ป้า_สำหรับ_อาหาร',\n",
       " 'สำหรับ_อาหาร_แต่',\n",
       " 'อาหาร_แต่_วัน',\n",
       " 'แต่_วัน_นั้น',\n",
       " 'วัน_นั้น_เจอ',\n",
       " 'นั้น_เจอ_เหตุการณ์',\n",
       " 'เจอ_เหตุการณ์_คุณ',\n",
       " 'เหตุการณ์_คุณ_ป้า',\n",
       " 'คุณ_ป้า_ให้',\n",
       " 'ป้า_ให้_อาหาร',\n",
       " 'ให้_อาหาร_แมว',\n",
       " 'อาหาร_แมว_ระหว่าง',\n",
       " 'แมว_ระหว่าง_ทำ',\n",
       " 'ระหว่าง_ทำ_ปอเปี๊ยะ',\n",
       " 'ทำ_ปอเปี๊ยะ_ไม่',\n",
       " 'ปอเปี๊ยะ_ไม่_ใส่',\n",
       " 'ไม่_ใส่_ถุง',\n",
       " 'ใส่_ถุง_มือ',\n",
       " 'ถุง_มือ_และ',\n",
       " 'มือ_และ_ไม่',\n",
       " 'และ_ไม่_เห็น',\n",
       " 'ไม่_เห็น_ว่า',\n",
       " 'เห็น_ว่า_ล้าง',\n",
       " 'ว่า_ล้าง_มือ',\n",
       " 'ล้าง_มือ_ผม',\n",
       " 'มือ_ผม_เลย',\n",
       " 'ผม_เลย_จำเป็น',\n",
       " 'เลย_จำเป็น_ต้อง',\n",
       " 'จำเป็น_ต้อง_หัก',\n",
       " 'ต้อง_หัก_ออก',\n",
       " 'หัก_ออก_อีก',\n",
       " 'ออก_อีก_หนึ่ง',\n",
       " 'อีก_หนึ่ง_ดาว']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['bigrams2'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T06:03:14.879636Z",
     "start_time": "2020-03-22T06:03:14.541992Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "def get_lda_input(data_words):\n",
    "    \n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "        \n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in data_words]\n",
    "    \n",
    "    return id2word, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T06:03:14.907562Z",
     "start_time": "2020-03-22T06:03:14.881630Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "def run_simple_model(list_of_data_words):\n",
    "    \n",
    "    for data_words in list_of_data_words:\n",
    "\n",
    "        id2word, corpus = get_lda_input(data_words)\n",
    "\n",
    "        # Build LDA model\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                               id2word=id2word,\n",
    "                                               num_topics=10, \n",
    "                                               random_state=100,\n",
    "                                               chunksize=100,\n",
    "                                               passes=10,\n",
    "                                               per_word_topics=True)\n",
    "\n",
    "        # Compute Coherence Score\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=id2word, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "        print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T06:04:19.413379Z",
     "start_time": "2020-03-22T06:03:16.216472Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.4845599722051414\n"
     ]
    }
   ],
   "source": [
    "run_simple_model([data_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T16:05:25.560758Z",
     "start_time": "2020-03-14T16:05:25.551813Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(texts, corpus, id2word, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b,\n",
    "                                           per_word_topics=True)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T16:05:25.581700Z",
     "start_time": "2020-03-14T16:05:25.567734Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "def grid_search(data_words, ofile_nm):\n",
    "    \n",
    "    id2word, corpus = get_lda_input(data_words)\n",
    "    print('finish get_lda_input')\n",
    "\n",
    "    grid = {}\n",
    "    grid['Validation_Set'] = {}\n",
    "\n",
    "    # Topics range\n",
    "    min_topics = 2\n",
    "    max_topics = 11\n",
    "    step_size = 1\n",
    "    topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "    # Alpha parameter\n",
    "    alpha = list(np.arange(0.01, 1, 0.3))\n",
    "    alpha.append('symmetric')\n",
    "    alpha.append('asymmetric')\n",
    "\n",
    "    # Beta parameter\n",
    "    beta = list(np.arange(0.01, 1, 0.3))\n",
    "    beta.append('symmetric')\n",
    "\n",
    "    # Validation sets\n",
    "    num_of_docs = len(corpus)\n",
    "    corpus_sets = [corpus]\n",
    "    corpus_title = ['100% Corpus']\n",
    "    model_results = {'Validation_Set': [],\n",
    "                     'Topics': [],\n",
    "                     'Alpha': [],\n",
    "                     'Beta': [],\n",
    "                     'Coherence': []\n",
    "                    }\n",
    "    \n",
    "    print('finish initial storage list')\n",
    "    \n",
    "    # Can take a long time to run\n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    try:\n",
    "                        cv = compute_coherence_values(corpus=corpus_sets[i], id2word=id2word, texts=data_words,\n",
    "                                                      k=k, a=a, b=b)\n",
    "                        # Save the model results\n",
    "                        model_results['Validation_Set'].append(corpus_title[i])\n",
    "                        model_results['Topics'].append(k)\n",
    "                        model_results['Alpha'].append(a)\n",
    "                        model_results['Beta'].append(b)\n",
    "                        model_results['Coherence'].append(cv)\n",
    "\n",
    "                        print(f'finish loop k : {k}, a : {a}, b : {b}, score : {cv:.4f}')\n",
    "                    except:\n",
    "                        print(f'error in loop k : {k}, a : {a}, b : {b}, score : {cv:.4f}')\n",
    "\n",
    "    pd.DataFrame(model_results).to_csv(ofile_nm, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T19:02:22.254279Z",
     "start_time": "2020-03-14T16:05:25.585688Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search(data_words_trigrams, ofile_nm='tuning_data_words_trigram_5K_docs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T06:28:00.550650Z",
     "start_time": "2020-03-15T06:27:55.990566Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = pd.read_csv('tuning_data_words_trigram_5K_docs.csv')\n",
    "_.sort_values('Coherence',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T06:29:59.459364Z",
     "start_time": "2020-03-15T06:29:59.340297Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T06:30:17.977286Z",
     "start_time": "2020-03-15T06:30:17.685514Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(x='Alpha', y='Coherence', data=_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T06:30:18.274380Z",
     "start_time": "2020-03-15T06:30:17.979280Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='Topics', y='Coherence', data=_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T06:30:18.777154Z",
     "start_time": "2020-03-15T06:30:18.539717Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='Beta', y='Coherence', data=_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T06:05:11.280130Z",
     "start_time": "2020-03-22T06:05:09.294035Z"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "\n",
    "try:\n",
    "    mlflow.create_experiment(name='lda_development')\n",
    "except:\n",
    "    expr = mlflow.get_experiment_by_name('lda_development')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-22T06:06:05.686182Z",
     "start_time": "2020-03-22T06:05:17.900898Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id2word, corpus = get_lda_input(data_words)\n",
    "k = 16\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=k,\n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)\n",
    "joblib.dump(lda_model, 'test_10Kdocs_30topics_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-22T06:06:57.879Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lda_10K = pyLDAvis.gensim.prepare(topic_model=lda_model, corpus=corpus, dictionary=id2word, n_jobs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(lda_10K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit [BOW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T13:01:59.282626Z",
     "start_time": "2020-03-07T13:01:50.476174Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T13:01:59.445452Z",
     "start_time": "2020-03-07T13:01:59.338552Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processed_docs = train['word_tokenized_uniq_cls_revstopw'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T13:01:59.636259Z",
     "start_time": "2020-03-07T13:01:59.450440Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T13:01:59.727681Z",
     "start_time": "2020-03-07T13:01:59.707734Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T13:01:59.830591Z",
     "start_time": "2020-03-07T13:01:59.765622Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T13:01:59.907294Z",
     "start_time": "2020-03-07T13:01:59.885511Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind = 500\n",
    "bow_doc = bow_corpus[ind]\n",
    "for i in range(len(bow_doc)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc[i][0], \n",
    "                                               dictionary[bow_doc[i][0]], \n",
    "bow_doc[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T13:01:59.988281Z",
     "start_time": "2020-03-07T13:01:59.909743Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T13:04:05.177485Z",
     "start_time": "2020-03-07T13:01:59.990454Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary,\n",
    "                                       passes=100, workers=2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T13:04:05.187458Z",
     "start_time": "2020-03-07T13:04:05.179480Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('\\n')\n",
    "    print('Topic: {} \\n@Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T13:04:27.117354Z",
     "start_time": "2020-03-07T13:04:05.191447Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=30, id2word=dictionary,\n",
    "                                             passes=20, workers=4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T13:04:27.127294Z",
     "start_time": "2020-03-07T13:04:27.118352Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('\\n')\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T13:04:27.670632Z",
     "start_time": "2020-03-07T13:04:27.129289Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ind in range(0,500,75):\n",
    "    print(\"*\"*30,ind,\"*\"*30)\n",
    "    print([train['review'][ind]])\n",
    "    for index, score in sorted(lda_model[bow_corpus[ind]], key=lambda tup: -1*tup[1]):\n",
    "        print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T13:04:27.721528Z",
     "start_time": "2020-03-07T13:04:27.698592Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ind in range(0,500,75):\n",
    "    print(\"*\"*30,ind,\"*\"*30)\n",
    "    print([train['review'][ind]])\n",
    "    for index, score in sorted(lda_model_tfidf[bow_corpus[ind]], key=lambda tup: -1*tup[1]):\n",
    "        print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
